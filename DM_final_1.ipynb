{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"train_nor_811.xlsx\"\n",
    "valid_filename = \"valid_nor_811.xlsx\"\n",
    "test_filename = \"test_nor_811.xlsx\"\n",
    "train_data = pd.read_excel(train_filename, engine = \"openpyxl\")\n",
    "valid_data = pd.read_excel(valid_filename, engine = \"openpyxl\")\n",
    "test_data = pd.read_excel(test_filename, engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def file_processing(data):\n",
    "    data.drop(columns = {\"Unnamed: 0\"}, axis = 1, inplace = True)\n",
    "    data[\"emotion_encode\"] = data[\"Emotion\"]\n",
    "    encoder = LabelEncoder()\n",
    "    data.emotion_encode = encoder.fit_transform(data.Emotion)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = file_processing(train_data)\n",
    "valid_data = file_processing(valid_data)\n",
    "test_data = file_processing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(word):\n",
    "    prev_char = \"\"\n",
    "    clean_word = \"\"\n",
    "    for character in word:\n",
    "        if(character != prev_char):\n",
    "            clean_word += character\n",
    "            prev_char = character\n",
    "    return clean_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def normalize_sentences(sentences):\n",
    "    punc_lst = {'.', ',', '...', '-', '“', '”', ':', '(', ')', '\"', '!', '&', ';', '?', '*', ']', '>', '…', '’',\"``\",\"''\", \"=\", \"%\", \"^\", \"@\", \"<\", \">\"}\n",
    "    confusing_words = {\"per\"}\n",
    "    acronym_word = {\n",
    "        \"ko\" : \"không\",\n",
    "        \"k\" : \"không\",\n",
    "        \"z\" : \"vậy\",\n",
    "        \"v\" : \"vậy\",\n",
    "        \"dzậy\" : \"vậy\",\n",
    "        \"dậy\": \"vậy\",\n",
    "        \"t\" : \"tao\",\n",
    "        \"m\" : \"mày\",\n",
    "        \"sgk\" : \"sách_giáo_khoa\",\n",
    "        \"zi\" : \"vậy\",\n",
    "        \"dth\" : \"dễ_thương\",\n",
    "        \"dume\": \"đụ mẹ\"\n",
    "    }\n",
    "    \n",
    "    clean_sentences = []\n",
    "    \n",
    "    # remove punctuation and lowercase\n",
    "    for sent in sentences:\n",
    "        \n",
    "        # remove emojis\n",
    "        sent = deEmojify(sent)\n",
    "        \n",
    "        sent = nltk.word_tokenize(sent)\n",
    "        temp = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            word = remove_duplicate(word)\n",
    "            if (word in punc_lst or word in confusing_words):\n",
    "                continue\n",
    "            elif(word in acronym_word):\n",
    "                temp.append(acronym_word[word])\n",
    "            elif(word.isdigit()):\n",
    "                temp.append(\"<NUM>\")\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        # remove whitespace\n",
    "        sent = ' '.join(temp)\n",
    "        \n",
    "        clean_sentences.append(sent)\n",
    "        \n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(ViTokenizer.tokenize(data.Sentence[i]))\n",
    "    \n",
    "    sentences = normalize_sentences(sentences)\n",
    "    encode_tags = data.emotion_encode\n",
    "    \n",
    "    # remove empty sentences\n",
    "    for idx, sent in enumerate(sentences):\n",
    "        if sent.strip() == \"\":\n",
    "            del sentences[idx]\n",
    "            del encode_tags[idx]\n",
    "    \n",
    "    return sentences, encode_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_sentences, train_encode_tags = normalize_dataset(train_data)\n",
    "valid_clean_sentences, valid_encode_tags = normalize_dataset(valid_data)\n",
    "test_clean_sentences, test_encode_tags = normalize_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import layers, activations , models , preprocessing , utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "\n",
    "tokenizer = Tokenizer(lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(train_clean_sentences + valid_clean_sentences)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_clean_sentences)\n",
    "X_train = pad_sequences(X_train, MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_valid = tokenizer.texts_to_sequences(valid_clean_sentences)\n",
    "X_valid = pad_sequences(X_valid, MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "MAX_LEN = 150\n",
    "w2v_model = Word2Vec(train_clean_sentences + valid_clean_sentences, min_count = 1, size = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2vec(sentence):\n",
    "    _sum = np.array([0]*MAX_LEN)\n",
    "    for word in sentence:\n",
    "        if not(word in w2v_model.wv.vocab):\n",
    "            continue\n",
    "        vec = w2v_model.wv[word]\n",
    "        _sum = _sum + vec\n",
    "    return _sum/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sent in train_clean_sentences:\n",
    "    if len(sent) == 0:\n",
    "        X_train.append(np.array([0]*MAX_LEN))\n",
    "    else:\n",
    "        X_train.append(convert2vec(sent))\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_valid = []\n",
    "for sent in valid_clean_sentences:\n",
    "    if len(sent) == 0:\n",
    "        X_valid.append(np.array([0]*MAX_LEN))\n",
    "    else:\n",
    "        X_valid.append(convert2vec(sent))\n",
    "X_valid = np.array(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "inputs = layers.Input(shape=( MAX_LEN , ))\n",
    "embedding = layers.Embedding(vocab_size, embedding_dim, input_length=MAX_LEN)(inputs)\n",
    "\n",
    "cnn1 = layers.Conv1D(filters=100, kernel_size=1, activation='relu')(embedding)\n",
    "cnn1 = layers.MaxPooling1D(pool_size=2)(cnn1)\n",
    "cnn1 = Flatten()(cnn1)\n",
    "\n",
    "cnn2 = layers.Conv1D(filters=100, kernel_size=2, activation='relu')(embedding)\n",
    "cnn2 = layers.MaxPooling1D(pool_size=2)(cnn2)\n",
    "cnn2 = Flatten()(cnn2)\n",
    "\n",
    "outputs = layers.Concatenate()([cnn1,cnn2])\n",
    "\n",
    "outputs = layers.Dense(28, activation='tanh')(outputs)\n",
    "outputs = layers.Dense(14, activation='tanh')(outputs)\n",
    "outputs = layers.Dense(7, activation='softmax')(outputs)\n",
    "model=models.Model(inputs,outputs)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(train_encode_tags)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_encode_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 11s 62ms/step - loss: 0.3698 - accuracy: 0.3211 - val_loss: 0.3281 - val_accuracy: 0.4548\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 10s 58ms/step - loss: 0.2843 - accuracy: 0.5448 - val_loss: 0.3196 - val_accuracy: 0.4723\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 10s 60ms/step - loss: 0.1899 - accuracy: 0.7420 - val_loss: 0.3327 - val_accuracy: 0.4752\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 10,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model2.add(SpatialDropout1D(0.2))\n",
    "model2.add(LSTM(128))\n",
    "model2.add(Dense(128, activation='sigmoid'))\n",
    "model2.add(Dense(7, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "87/87 [==============================] - 31s 355ms/step - loss: 1.8035 - accuracy: 0.2719 - val_loss: 1.7398 - val_accuracy: 0.3120\n",
      "Epoch 2/20\n",
      "87/87 [==============================] - 28s 318ms/step - loss: 1.7802 - accuracy: 0.2809 - val_loss: 1.7703 - val_accuracy: 0.3120\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "y_train_lstm = tf.keras.utils.to_categorical(train_encode_tags)\n",
    "y_valid_lstm = tf.keras.utils.to_categorical(valid_encode_tags)\n",
    "\n",
    "history2 = model2.fit(X_train, y_train_lstm,\n",
    "                    epochs = 20,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid_lstm),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model = Sequential()\n",
    "bi_model.add(Embedding(vocab_size, embedding_dim, input_length=MAX_LEN))\n",
    "bi_model.add(SpatialDropout1D(0.2))\n",
    "bi_model.add(layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "bi_model.add(Dense(7, activation='softmax'))\n",
    "bi_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 353s 2s/step - loss: 1.6882 - accuracy: 0.3314 - val_loss: 1.5039 - val_accuracy: 0.4227\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 311s 2s/step - loss: 1.3185 - accuracy: 0.5102 - val_loss: 1.3867 - val_accuracy: 0.4621\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 341s 2s/step - loss: 1.0267 - accuracy: 0.6270 - val_loss: 1.2826 - val_accuracy: 0.5219\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 363s 2s/step - loss: 0.8158 - accuracy: 0.7087 - val_loss: 1.3944 - val_accuracy: 0.5175\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "bi_history = bi_model.fit(X_train, y_train_lstm,\n",
    "                    epochs=10,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid_lstm),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_model.save(\"bi_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Decision Tree Model + TF IDF vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF vectorize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf = True, max_features=1800)\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(train_clean_sentences)\n",
    "\n",
    "tfidf_vectorizer_vectors_test = tfidf_vectorizer.fit_transform(test_clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547, 1800)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier()\n",
    "Y_DT = np.array(train_encode_tags)\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(tfidf_vectorizer_vectors):\n",
    "    X_train_DT, X_test_DT = tfidf_vectorizer_vectors[train_index], tfidf_vectorizer_vectors[test_index]\n",
    "    y_train_DT, y_test_DT = Y_DT[train_index], Y_DT[test_index]\n",
    "    \n",
    "    tree_model.fit(X_train_DT, y_train_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_DT = tree_model.predict(tfidf_vectorizer_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.07      0.10        40\n",
      "           1       0.18      0.38      0.24       132\n",
      "           2       0.33      0.22      0.26       193\n",
      "           3       0.09      0.02      0.04        46\n",
      "           4       0.19      0.16      0.18       129\n",
      "           5       0.17      0.16      0.17       116\n",
      "           6       0.06      0.05      0.06        37\n",
      "\n",
      "    accuracy                           0.20       693\n",
      "   macro avg       0.17      0.15      0.15       693\n",
      "weighted avg       0.21      0.20      0.19       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_encode_tags, y_pred_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Linear SVC + TF IDF vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC()\n",
    "clf = linear_svc.fit(tfidf_vectorizer_vectors, train_encode_tags)\n",
    "\n",
    "svc_model = CalibratedClassifierCV(base_estimator=linear_svc, cv=\"prefit\")\n",
    "\n",
    "svc_model.fit(tfidf_vectorizer_vectors, train_encode_tags)\n",
    "y_pred_DT = svc_model.predict(tfidf_vectorizer_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.07      0.09        40\n",
      "           1       0.24      0.33      0.28       132\n",
      "           2       0.27      0.35      0.30       193\n",
      "           3       0.00      0.00      0.00        46\n",
      "           4       0.19      0.16      0.17       129\n",
      "           5       0.17      0.14      0.15       116\n",
      "           6       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.22       693\n",
      "   macro avg       0.14      0.15      0.14       693\n",
      "weighted avg       0.19      0.22      0.20       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_encode_tags, y_pred_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
