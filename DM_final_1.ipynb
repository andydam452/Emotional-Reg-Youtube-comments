{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"train_nor_811.xlsx\"\n",
    "valid_filename = \"valid_nor_811.xlsx\"\n",
    "test_filename = \"test_nor_811.xlsx\"\n",
    "train_data = pd.read_excel(train_filename, engine = \"openpyxl\")\n",
    "valid_data = pd.read_excel(valid_filename, engine = \"openpyxl\")\n",
    "test_data = pd.read_excel(test_filename, engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def file_processing(data):\n",
    "    data.drop(columns = {\"Unnamed: 0\"}, axis = 1, inplace = True)\n",
    "    data[\"emotion_encode\"] = data[\"Emotion\"]\n",
    "    encoder = LabelEncoder()\n",
    "    data.emotion_encode = encoder.fit_transform(data.Emotion)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = file_processing(train_data)\n",
    "valid_data = file_processing(valid_data)\n",
    "test_data = file_processing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(word):\n",
    "    prev_char = \"\"\n",
    "    clean_word = \"\"\n",
    "    for character in word:\n",
    "        if(character != prev_char):\n",
    "            clean_word += character\n",
    "            prev_char = character\n",
    "    return clean_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def normalize_sentences(sentences):\n",
    "    punc_lst = {'.', ',', '...', '-', '“', '”', ':', '(', ')', '\"', '!', '&', ';', '?', '*', ']', '>', '…', '’',\"``\",\"''\", \"=\", \"%\", \"^\", \"@\", \"<\", \">\"}\n",
    "    confusing_words = {\"per\"}\n",
    "    acronym_word = {\n",
    "        \"ko\" : \"không\",\n",
    "        \"k\" : \"không\",\n",
    "        \"z\" : \"vậy\",\n",
    "        \"v\" : \"vậy\",\n",
    "        \"dzậy\" : \"vậy\",\n",
    "        \"dậy\": \"vậy\",\n",
    "        \"t\" : \"tao\",\n",
    "        \"m\" : \"mày\",\n",
    "        \"sgk\" : \"sách_giáo_khoa\",\n",
    "        \"zi\" : \"vậy\",\n",
    "        \"dth\" : \"dễ_thương\",\n",
    "        \"dume\": \"đụ mẹ\"\n",
    "    }\n",
    "    \n",
    "    clean_sentences = []\n",
    "    \n",
    "    # remove punctuation and lowercase\n",
    "    for sent in sentences:\n",
    "        \n",
    "        # remove emojis\n",
    "        sent = deEmojify(sent)\n",
    "        \n",
    "        sent = nltk.word_tokenize(sent)\n",
    "        temp = []\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            word = remove_duplicate(word)\n",
    "            if (word in punc_lst or word in confusing_words):\n",
    "                continue\n",
    "            elif(word in acronym_word):\n",
    "                temp.append(acronym_word[word])\n",
    "            elif(word.isdigit()):\n",
    "                temp.append(\"<NUM>\")\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        # remove whitespace\n",
    "        sent = ' '.join(temp)\n",
    "        \n",
    "        clean_sentences.append(sent)\n",
    "        \n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(data):\n",
    "    sentences = []\n",
    "    for i in range(len(data)):\n",
    "        sentences.append(ViTokenizer.tokenize(data.Sentence[i]))\n",
    "    \n",
    "    sentences = normalize_sentences(sentences)\n",
    "    encode_tags = data.emotion_encode\n",
    "    \n",
    "    # remove empty sentences\n",
    "    for idx, sent in enumerate(sentences):\n",
    "        if sent.strip() == \"\":\n",
    "            del sentences[idx]\n",
    "            del encode_tags[idx]\n",
    "    \n",
    "    return sentences, encode_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_sentences, train_encode_tags = normalize_dataset(train_data)\n",
    "valid_clean_sentences, valid_encode_tags = normalize_dataset(valid_data)\n",
    "test_clean_sentences, test_encode_tags = normalize_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import layers, activations , models , preprocessing , utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "\n",
    "tokenizer = Tokenizer(lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts(train_clean_sentences + valid_clean_sentences)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_clean_sentences)\n",
    "X_train = pad_sequences(X_train, MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_valid = tokenizer.texts_to_sequences(valid_clean_sentences)\n",
    "X_valid = pad_sequences(X_valid, MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test_clean_sentences)\n",
    "X_test = pad_sequences(X_test, MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "MAX_LEN = 150\n",
    "w2v_model = Word2Vec(train_clean_sentences + valid_clean_sentences, min_count = 1, size = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2vec(sentence):\n",
    "    _sum = np.array([0]*MAX_LEN)\n",
    "    for word in sentence:\n",
    "        if not(word in w2v_model.wv.vocab):\n",
    "            continue\n",
    "        vec = w2v_model.wv[word]\n",
    "        _sum = _sum + vec\n",
    "    return _sum/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sent in train_clean_sentences:\n",
    "    if len(sent) == 0:\n",
    "        X_train.append(np.array([0]*MAX_LEN))\n",
    "    else:\n",
    "        X_train.append(convert2vec(sent))\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_valid = []\n",
    "for sent in valid_clean_sentences:\n",
    "    if len(sent) == 0:\n",
    "        X_valid.append(np.array([0]*MAX_LEN))\n",
    "    else:\n",
    "        X_valid.append(convert2vec(sent))\n",
    "X_valid = np.array(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf = True, max_features = 3000)\n",
    "\n",
    "fitted_vectorizer = tfidf_vectorizer.fit(train_clean_sentences)\n",
    "\n",
    "X_train = fitted_vectorizer.transform(train_clean_sentences).toarray()\n",
    "\n",
    "X_valid = fitted_vectorizer.transform(valid_clean_sentences).toarray()\n",
    "\n",
    "X_test = fitted_vectorizer.transform(test_clean_sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "inputs = layers.Input(shape=( MAX_LEN , ))\n",
    "embedding = layers.Embedding(vocab_size, embedding_dim, input_length=MAX_LEN)(inputs)\n",
    "\n",
    "cnn1 = layers.Conv1D(filters=100, kernel_size=1, activation='relu')(embedding)\n",
    "cnn1 = layers.MaxPooling1D(pool_size=2)(cnn1)\n",
    "cnn1 = Flatten()(cnn1)\n",
    "\n",
    "cnn2 = layers.Conv1D(filters=100, kernel_size=2, activation='relu')(embedding)\n",
    "cnn2 = layers.MaxPooling1D(pool_size=2)(cnn2)\n",
    "cnn2 = Flatten()(cnn2)\n",
    "\n",
    "outputs = layers.Concatenate()([cnn1,cnn2])\n",
    "\n",
    "outputs = layers.Dense(28, activation='tanh')(outputs)\n",
    "outputs = layers.Dense(14, activation='tanh')(outputs)\n",
    "outputs = layers.Dense(7, activation='softmax')(outputs)\n",
    "model=models.Model(inputs,outputs)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(train_encode_tags)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_encode_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 131s 751ms/step - loss: 0.3900 - accuracy: 0.2627 - val_loss: 0.3767 - val_accuracy: 0.3120\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 127s 730ms/step - loss: 0.3825 - accuracy: 0.2809 - val_loss: 0.3759 - val_accuracy: 0.3120\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 126s 725ms/step - loss: 0.3822 - accuracy: 0.2809 - val_loss: 0.3760 - val_accuracy: 0.3120\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 10,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.10      0.14        40\n",
      "           1       0.46      0.55      0.50       132\n",
      "           2       0.52      0.63      0.57       193\n",
      "           3       0.71      0.54      0.62        46\n",
      "           4       0.38      0.42      0.40       129\n",
      "           5       0.54      0.52      0.53       116\n",
      "           6       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.48       693\n",
      "   macro avg       0.41      0.39      0.39       693\n",
      "weighted avg       0.46      0.48      0.47       693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_encode_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-56e9b31dbf88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# model2.add(SpatialDropout1D(0.2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model2.add(SpatialDropout1D(0.2))\n",
    "model2.add(LSTM(128))\n",
    "model2.add(Dense(128, activation='sigmoid'))\n",
    "model2.add(Dense(7, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "87/87 [==============================] - 31s 355ms/step - loss: 1.8035 - accuracy: 0.2719 - val_loss: 1.7398 - val_accuracy: 0.3120\n",
      "Epoch 2/20\n",
      "87/87 [==============================] - 28s 318ms/step - loss: 1.7802 - accuracy: 0.2809 - val_loss: 1.7703 - val_accuracy: 0.3120\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "y_train_lstm = tf.keras.utils.to_categorical(train_encode_tags)\n",
    "y_valid_lstm = tf.keras.utils.to_categorical(valid_encode_tags)\n",
    "\n",
    "history2 = model2.fit(X_train, y_train,\n",
    "                    epochs = 20,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5ac429c71301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbi_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSpatialDropout1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "bi_model = Sequential()\n",
    "bi_model.add(Embedding(vocab_size, embedding_dim, input_length=MAX_LEN))\n",
    "bi_model.add(SpatialDropout1D(0.2))\n",
    "bi_model.add(layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "bi_model.add(Dense(7, activation='softmax'))\n",
    "bi_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 317s 2s/step - loss: 1.7038 - accuracy: 0.3270 - val_loss: 1.5055 - val_accuracy: 0.4329\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 356s 2s/step - loss: 1.2973 - accuracy: 0.5125 - val_loss: 1.3263 - val_accuracy: 0.5058\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 322s 2s/step - loss: 1.0063 - accuracy: 0.6387 - val_loss: 1.2990 - val_accuracy: 0.5175\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 296s 2s/step - loss: 0.8090 - accuracy: 0.7119 - val_loss: 1.3786 - val_accuracy: 0.5058\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "bi_history = bi_model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bi_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-f870a37e6312>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bi_model_tfidf.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bi_model' is not defined"
     ]
    }
   ],
   "source": [
    "bi_model.save(\"bi_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bi_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.38      0.42        40\n",
      "           1       0.49      0.64      0.55       132\n",
      "           2       0.63      0.56      0.59       193\n",
      "           3       0.61      0.61      0.61        46\n",
      "           4       0.43      0.41      0.42       129\n",
      "           5       0.52      0.64      0.57       116\n",
      "           6       0.78      0.19      0.30        37\n",
      "\n",
      "    accuracy                           0.53       693\n",
      "   macro avg       0.56      0.49      0.50       693\n",
      "weighted avg       0.55      0.53      0.53       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_encode_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Dense + TF IDF vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(layers.Dense(512, activation = \"relu\"))\n",
    "model_3.add(layers.Dense(128, activation = \"sigmoid\"))\n",
    "model_3.add(layers.Dense(7, activation = \"softmax\"))\n",
    "model_3.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.7517 - accuracy: 0.2751 - val_loss: 1.5560 - val_accuracy: 0.4621\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 1s 16ms/step - loss: 1.2672 - accuracy: 0.5596 - val_loss: 1.2702 - val_accuracy: 0.5175\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 1s 15ms/step - loss: 0.8541 - accuracy: 0.7184 - val_loss: 1.2993 - val_accuracy: 0.5277\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(train_encode_tags)\n",
    "y_valid = tf.keras.utils.to_categorical(valid_encode_tags)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "history_3 = model_3.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    callbacks = [es],\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.15      0.21        40\n",
      "           1       0.50      0.68      0.58       132\n",
      "           2       0.58      0.59      0.58       193\n",
      "           3       0.79      0.59      0.68        46\n",
      "           4       0.43      0.48      0.46       129\n",
      "           5       0.54      0.56      0.55       116\n",
      "           6       1.00      0.08      0.15        37\n",
      "\n",
      "    accuracy                           0.53       693\n",
      "   macro avg       0.60      0.45      0.46       693\n",
      "weighted avg       0.55      0.53      0.51       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_encode_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Linear SVC + TF IDF vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC()\n",
    "clf = linear_svc.fit(X_train, train_encode_tags)\n",
    "\n",
    "svc_model = CalibratedClassifierCV(base_estimator=linear_svc, cv=\"prefit\")\n",
    "\n",
    "svc_model.fit(X_train, train_encode_tags)\n",
    "y_pred = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.30      0.29        40\n",
      "           1       0.51      0.54      0.53       132\n",
      "           2       0.56      0.60      0.58       193\n",
      "           3       0.64      0.63      0.64        46\n",
      "           4       0.48      0.45      0.46       129\n",
      "           5       0.55      0.54      0.55       116\n",
      "           6       0.58      0.41      0.48        37\n",
      "\n",
      "    accuracy                           0.52       693\n",
      "   macro avg       0.51      0.49      0.50       693\n",
      "weighted avg       0.53      0.52      0.52       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_encode_tags, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
